# 主要内容

## 重新格式化prompt

> 为了让模型能够输出短的或者长的输出。明确指示输出格式，在提示简短答案时附加在 VQA 问题的末尾：使用单个单词或短语或详细的回答问题。

## 将 Linear 改为 MLP

## 使用面向学术任务的 VQA 数据集，用于 VQA、OCR 和区域级感知，以多种方式增强模型的能力

## 将分辨率由224提高到336，模型使用 CLIP-ViT-L-336px

## 使用了更大的语言模型

## 尝试更高的分辨率

### 动态高分辨率

我们以高分辨率设计模型，以保持**其数据效率**。当提供高分辨率图像和保留这些细节的表示时，模型感知图像中复杂细节的能力会显着提高。它减少了模型幻觉，当面对低分辨率图像时，这种幻觉会推测想象的视觉内容。我们的“AnyRes”技术旨在适应各种高分辨率的图像。我们采用的网格配置为 `{2×2,1×{2,3,4}，{2,3,4}×1}`，平衡性能效率和运营成本

![img](2310.03744%20LLaVA%201.5%20%E6%96%B0%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A.assets/high_res_arch_v2.png)

*动态高分辨率方案示意图：网格配置2×2*

> 使用 224 * 224 分辨率作为例子
>
> 分割后的patch的feature会被合并回单个大特征图，然后扁平化，最后拼接resize的全局特征图的扁平化数据。

我们使用 CLIP-ViT-L-14 (224 * 224) 作为基础图像编码器。我们首先选择输入图像并将其填充到能够有效捕获其细节的目标分辨率，并将图像分割为 224 * 224 网格。所有 224 * 224 图像块均由 CLIP 图像编码器单独编码，并且它们的特征被合并回单个大特征图。

### 后处理

我们执行三个后处理步骤，以确保最终的特征能够被语言模型有效且高效地处理。

(1) 去除填充。专门对应于填充的特征将被丢弃。这减少了语言模型处理的视觉标记的数量并提高了效率。

(2) 行结束标记。我们在每行特征的末尾附加一个特殊的标记，以提供图像形状的明确指示。与原始的LLaVA和LLaVA-1.5使用固定分辨率不同，我们现在对LLaVA-1.5-HD的图像特征使用可变分辨率，这样的指示允许语言模型对每个样本捕获每个图像的精确形状和大小。

(3)扁平化。最后，我们将图像特征图展平并将其与语言token特征一起输入到语言模型中。

### 全局特征

最后，我们将生成的特征图后期处理为扁平化的特征列表。我们还拼接了固定分辨率图像的特征，为模型提供全局上下文。

## 效果

我们表明，LLaVA-1.5 在 12 个基准测试中实现了最佳整体性能，尽管与其他方法相比使用了更小的预训练和指令调整数据。LLaVA-1.5 在指令跟踪 LMM 的所有基准测试中均显着优于 LLaVA。请注意，在需要开放式简短答案的 VQA-v2 等学术数据集上评估原始 LLaVA 具有挑战性。

当我们继续使用 LLaVA-1.5-HD 将图像分辨率扩展到 448 * 448 时，它进一步提高了所有基准测试的整体性能，特别是在需要感知图像细节的任务上（例如 MM Vet 中的 OCR、详细 LLaVA-Bench-in-the-Wild 中的描述）。此外，我们发现添加全局上下文可以有效地从 split-and-merge artifacts 中恢复模型，并引导模型更轻松地从高分辨率（见附录）中定位相关区域。

结果还表明，视觉指令调整在提高 LMM 的能力方面发挥着重要作用，并提出了人们普遍认为 LMM 需要大量视觉语言对齐预训练的问题，尽管视觉编码器（例如 CLIP、OpenCLIP、 EVA-CLIP 等）已经在网络规模的图像文本配对数据上进行了预训练。这也让我们重新思考视觉采样器的好处以及额外的大规模预训练在多模式指令跟踪能力方面的必要性。

## 开放问题

### 数据效率，及训练效率

我们通过对 LLaVA-1.5 的训练数据混合物进行随机子采样来进一步提高数据效率，采样率范围为 0.1 至 0.5。我们在图 4 中可视化不同采样变量的相对性能。令我们惊讶的是，仅使用 50% 的样本，该模型仍然保持了完整数据集 98% 以上的性能。这表明数据效率还有进一步提高的空间。

### 幻觉

幻觉是 LLM 和 LMM 需要解决的一个重要问题。通常在 LMM 中，我们将模型的幻觉归因于训练数据集中的错误或幻觉。例如，LLaVA-Instruct 中的详细描述可能包含少量幻觉内容，人们认为，对此类数据的训练可能导致模型在被要求“详细描述图像”时产生幻觉。然而，我们发现，当我们将模型的输入缩放到更高分辨率（如 448 * 448）时，这种幻觉会显着减少。

这一发现很有趣，因为它表明 LMM 对于训练数据中的一些此类错误可能具有鲁棒性。然而，当输入分辨率不足以让模型识别训练数据中的所有细节，并且超出模型能力的粒度的数据量变得足够大时，模型就会学会产生幻觉。这进一步表明，在用更多细节改进数据注释和模型以这种粒度正确处理信息的能力之间需要取得平衡。不平衡的缩放可能会导致模型产生幻觉的倾向增加或对视觉细节的理解减少。我们希望这一发现为未来处理幻觉以及模型和数据的扩展方面的工作提供参考。

### 组合能力

我们在 LLaVA-1.5 中展示了有趣的组合功能：在一组任务上独立训练的模型可以推广到需要这些功能组合而无需明确联合训练的任务。我们注意到以下一些发现。

首先，我们观察到在包含 ShareGPT 数据后，视觉对话中的语言能力得到了改善，包括第 4.3 节中讨论的多模式多语言能力。此外，该模型更有能力在视觉对话中提供更长、更详细的响应。其次，来自学术任务导向数据集的额外视觉知识提高了 LLaVA-1.5 在视觉对话中反应的视觉基础性，MM Vet 和 LLaVA-Wild 的改进结果定量证明了这一点见表4。

然而，对于一些需要一定能力组合的任务来说，仍然很难达到理想的性能。例如，能够在VQA中正确回答某个对象的属性，并不能保证在整个图像的详细描述中准确描述该对象属性。此外，与某些外语（例如韩语）进行对话的能力仍然落后。示例请参见附录。

这些发现表明，可以利用 LMM 的组合能力来提高模型的性能，而无需通过详尽地包含所有任务组合来显着增加数据。然而，它还可以进一步研究，更深入地理解 LMM 的组合能力背后的机制可以进一步提高 LLaVA-1.5 的能力和数据效率。

## 缺点

### 无法处理多图片，缺少此种数据集