# 主要内容

Mixtral 是一个稀疏的专家混合模型。它是一个纯解码器模型，其中 FFN 块从 8 个不同的 FFN 中进行选择。在每一层，对于每个 token，路由器模型选择组中的两个 FFN（“专家”）来处理 token 并通过相加组合它们的输出。尽管每个 token 只看到两个专家，但所选的专家在每个时间步都可能不同。因此，每个 token 可以访问 47B 个参数，但在推理过程中仅使用 13B 个参数。

## 结构

![Mixture of Experts Layer](2401.04088%20Mixtral%20of%20Experts.assets/Mixture%20of%20Experts%20Layer.png)

每个输入向量由路由器分配给 8 个专家中的 2 个。该层的输出是两个所选专家输出的加权和。在 Mixtral 中，专家是标准 FFN，就像普通 Transformer 架构中一样。

<img src="2401.04088%20Mixtral%20of%20Experts.assets/architecture parameters.png" alt="image-20240304165102343" style="zoom:25%;" />

我们简要概述了专家混合层。有关更深入的概述，请参阅 Mistral 7B。对于给定输入 $x$，MoE 模块的输出由专家网络输出的加权和确定，其中权重由门控网络的输出给出。即给定 $n$ 个专家网络 $\{E_0, E_i , \dots, E_{n−1}\}$，专家层的输出由下式给出：
$$
\sum_{i=0}^{n-1}G(x)_i \cdot E_i(x)
$$
在这里，$G(x)_i$ 表示第 $i$ 个专家的门控网络的 $n$ 维输出，$E_i(x)$ 是第 $i$ 个专家网络的输出。如果门向量稀疏，我们可以避免门控网络输出为0的专家的输出。实现 $G(x)$ 有多种可行的方法，但一种简单且高性能的方法是通过在线性层的 Top-K logits 上采用 softmax 来实现。我们用
$$
G(x) := Softmax(TopK(x \cdot W_g))
$$
如果 $ℓ_i$ 位于 logits $ℓ ∈ R^n$ 的前 K 个坐标之中 $(TopK(ℓ))_i := ℓ_i$，否则 $(TopK(ℓ))_i := −∞$ 。$K$ 的值（每个 token 使用的专家数量）是一个超参数，用于调节处理每个 token 的计算量。如果在保持 $K$ 固定的同时增加 $n$，则可以增加模型的参数数量，同时保持其计算成本有效恒定。这使模型的总参数计数（通常称为“稀疏参数总数”，会随着 $n$ 的增加而增加）与用于处理单个 token 的参数数量（称为“活跃参数”，会随着 $K$ 增加而增加）之间存在区别。

MoE 层可以在具有高性能专用内核的单个 GPU 上高效运行。例如，Megablocks 将 MoE 层的前馈网络（FFN）操作转换为大型稀疏矩阵乘法，显着提高了执行速度，并自然地处理了不同的专家获得的 token 数量不同的情况。另外，MoE 层可以通过标准模型并行技术分布到多个 GPU，这里使用一种称为专家并行 (EP) 的特殊分片策略。在 MoE 层执行期间，本应由特定专家处理的 token 被分配到相应的 GPU 进行处理，之后专家的输出被返回到原始 token 位置。请注意，EP 在负载平衡方面带来了挑战，因为必须在 GPU 之间均匀分配工作负载，以防止单个 GPU 过载或遇到计算瓶颈。

在 Transformer 模型中，MoE 层独立应用于每个 token，并取代 Transformer 块的前馈 (FFN) 子块。在 Mixtral 中，我们使用 SwiGLU 架构作为专家函数 $E_i(x)$ ，并设置 $K = 2$。这意味着每个 token 都被路由到具有不同权重的两个 SwiGLU 子块。将上面所有模块放在一起后，输入 token $x$ ,输出 $y$ 的计算如下：
$$
y = \sum_{i=0}^{n-1} Softmax(Top2(x \cdot W_g))_i \cdot SwiGLU_i(x)
$$


This formulation is similar to the GShard architecture, with the exceptions that we replace all FFN sub-blocks by MoE layers while GShard replaces every other block, and that GShard uses a more elaborate gating strategy for the second expert assigned to each token.

这个公式与 GShard 架构类似，不同之处在于我们用 MoE 层替换所有 FFN 子块，而 GShard 替换所有其他块（不懂，看GShared论文也是用 MoE 替换 FFN，其他的比如 Attention 是共享的），并且 GShard 对分配给每个 token 的第二个专家使用更精细的门控策略。

## instruct 微调

在微调数据集上使用监督微调 (SFT)，然后在配对的反馈数据集上使用直接偏好优化 (DPO)。

## 路由分析

本节我们对路由器的专家选择进行一个小分析。我们特别有兴趣了解在训练期间是否有一些专家专门研究某些特定领域（例如数学、生物学、哲学等）。

![image-20240305101815091](2401.04088%20Mixtral%20of%20Experts.assets/route analysis.png)

（上图说明）将来自 The Pile 数据集分配给第 0、15 和 31 层不同领域的每位专家的 token 比例。灰色垂直虚线 token  1/8，即均匀采样的预期比例。在这里，我们考虑被路由器选为第一或第二的专家。

为了调查这一点，我们测量了所选专家在 Pile 验证数据集的不同子集上的分布。上图中显示了第 0、15 和 31 层的结果（第 0 层和第 31 层分别是模型的第一层和最后一层）。令人惊讶的是，我们在根据数据主题分配专家时没有观察到明显的模式。例如，在所有层中，ArXiv 论文（用 Latex 编写）、生物学（PubMed Abstracts）和哲学（PhilPapers）文档的专家分配分布非常相似。

仅对于 DM 数学，我们注意到专家的分布略有不同。这种差异可能是数据集的合成性质及其自然语言的有限覆盖（覆盖不全数学数据）的结果，并且在第一层和最后一层尤其明显，其中隐藏状态分别与输入和输出嵌入高度相关。

这表明路由器确实表现出一些结构化的语法行为。下图显示了来自不同领域（Python 代码、数学和英语）的文本示例，其中每个 token 都使用与其所选专家相对应的背景颜色突出显示。该图显示，Python 中的“self”和英语中的“Question”等单词经常通过同一位专家计算，即使它们涉及多个 token 。类似地，在代码中，缩进 token 始终分配给相同的专家，特别是在隐藏状态与模型的输入和输出更相关的第一层和最后一层。

![image-20240305103707640](2401.04088%20Mixtral%20of%20Experts.assets/python expert choice.png)

文本样本，其中每个 token 均采用第一个专家选择的颜色作为背景颜色。专家的选择似乎更符合语法而不是领域，特别是在初始层和最终层。

我们还从上图中注意到，连续的 token 通常被分配给相同的专家。事实上，我们在 The Pile 数据集中观察到一定程度的位置局部性。下表显示了在每个领域和每层中连续的 token 被分配给相同专家的比例。对于较高层，重复连续分配的比例明显高于随机分配的比例。这对于如何优化模型以进行快速训练和推理具有影响。例如，在进行专家并行时，局部性较高的文本更有可能导致某些专家的过载。反过来说，可以利用该位置进行缓存，就像 [11] 中所做的那样。附录中的图 10 为所有层和跨数据集提供了这些相同专家频率的完整视图。

![image-20240305105035949](2401.04088%20Mixtral%20of%20Experts.assets/Percentage of expert assignment repetitions.png)

连续 token 被分配给相同专家的百分比。我们评估同一位专家被分配给 token $i$ 及其后续 token $i+1$ 的次数比例。我们报告第一个被选择的专家是否相同，或者同一位专家是否被连续的 token 选择为第一或第二专家。作为参考，随机分配情况下的预期重复比例为 $\frac 1 8 = 12.5\%$ （“第一选择”）和 $1 − \frac 6 8 \frac 5 7 ≈ 46\%$ （“第一和第二选择”）。第一层的选择重复比例接近随机，但第 15 层和第 31 层的重复次数明显更高。大量的重复表明在这些层上连续的 token 选择专家时表现出较高的时间局部性。

