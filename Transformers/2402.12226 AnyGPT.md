# 主要内容



我们提出了 AnyGPT，这是一种任意模态到任意模态的多模态语言模型，采用离散的表示进行统一处理。AnyGPT 使用了多模态分词器，可将原始多模态数据（例如图像和音频）压缩为一系列离散语义的token。这些离散的表示使核心 LLM 能够在语义层面以自回归方式统一感知、理解、推理和生成等任务。随后，多模态分词解码器将离散表示转换回感知层面上的原始模态表示。由于离散表示可以过滤掉高频、特定模态的感知信息，同时保留基本的低频语义信息，因此我们可以稳定地训练我们的模型，而无需对现有的 LLM 架构或训练范式进行任何更改。我们的方法仅依赖于数据级预处理。这允许将新模态无缝集成到 LLM 中，类似于添加新语言，并允许直接使用现有的 LLM 工具，从而提高训练和推理阶段的效率。

此外，为了缓解包含所有模态的多模态对齐数据的稀缺性，我们构建了一个以文本为中心的多模态对齐数据集用于预训练。我们的目标是使用文本作为桥梁，通过将其他模态与文本对齐，实现所有模态之间的相互对齐，原因是自然语言是最精细的语义表示模态，并且存在于大多数多模态对齐数据集中。为了赋予模型理解和生成多种模态交织的内容的能力，我们采用先进的生成模型来合成多模态提示数据集：AnyInstruct-108k。该数据集包含 108k 多轮对话样本，使 AnyGPT 能够处理任意组合的多模态输入和输出。

## 主要结构

![image-20240304094231863](2402.12226%20AnyGPT.assets/AnyGPT architecture.png)

AnyGPT 模型架构概述。所有模态都通过多模态分词器被转换为离散标记，LLM 在此基础上执行多模态理解和自回归生成。只需要数据预处理和后处理，模型的架构和训练目标保持不变。

我们的兴趣在于通过 LLM 实现任何模态到任何模态的生成。为了实现这一点，我们提出了一个可以统一训练的综合框架。如上图所示，这个框架主要由3个部分组成：（1）多模态分词器、（2）一个多模态语言模型作为backbone、（3）多模态分词解码器。多模态分词器将连续的非文字模态转换为离散的 token，它们随后被放入一个混合多模态模态的交错序列。然后，语言模型使用预测下一个 token 的方式来训练序列。在推理阶段，多模态 token 通过分词解码器解码为原本的模态。为了增强生成模态的质量，模态增强模块可以应用在生成结果的后处理中，比如声音克隆或者图像超分辨率模块。在随后的章节中，我们会详细介绍每个模块。

### Tokenization

<img src="2402.12226%20AnyGPT.assets/tokenization.png" alt="image-20240304103129111" style="zoom: 33%;" />

#### Image Tokenizer

我们利用 SEED 分词器进行图像分词。SEED 分词器由多个组件组成，包括 ViT 编码器、Causal Q-Former、VQ Codebook、多层感知器 (MLP) 和 UNet 解码器。SEED 使用 224 × 224 RGB 的图像作为输入，ViT 编码器将图像编码为 16 × 16 个 patch，然后 Causal Q-Former 将 patch 特征转换为 32 个 casual embedding。包含 8192 个条目的 codebook 将 casual embedding 嵌入为量化编码序列。使用 MLP 将视觉量化编码序列转化为 generation embedding，该 embedding 与预训练的 unCLIP 稳定扩散模型的潜在空间对齐。最后，使用UNet解码器将 generation  embedding 还原为原始图像。

#### Speech Tokenizer

我们使用的语音分词器是 SpeechTokenizer，结构为采用带有残差矢量量化（RVQ）的编码器-解码器架构。SpeechTokenizer 使用八个分层量化器（每个量化器有 1,024 个条目，50 Hz 的帧速率）将单通道音频序列压缩为离散矩阵。第1个量化器捕捉语义内容，剩下的 2~8 个量化器编码副语言的细节。一个10秒钟的声音被转换为一个 500x8 的矩阵。分立为语义和声学的 token。我们使用了一个在 Commonvoice 和 Librispeech 数据集上预训练的 SpeechTokenizer 变体。

在 AnyGPT 中，大语言模型 (LLM) 对语义 token 进行建模，而语音克隆模型则补充剩余的副语言信息。因此，LLM 中语音词汇的大小等同于于一个 codebook 的大小，即 1024。

#### Music Tokenizer

尽管语音和音乐使用相似的数据格式，但它们在内容上的巨大差异导致我们将它们视为不同的模态，每种模态都配备了自己的分词器。对于音乐，我们采用 Encodec（一种卷积自编码器，其潜在空间使用残差矢量量化（RVQ）进行量化的模型）作为音乐分词器。我们使用现成的 Encodec1 变体，它在 20k 个音乐片段上进行了预训练。该变体使用 32 kHz 单声道音频和 50 Hz 的帧速率。生成的 embedding 使用带有四个量化器的 RVQ 进行量化，每个量化器的 codebook 大小为 2048，从而产生组合的音乐词汇大小为 8192。

我们将 5 秒的音乐编码为 250 个潜在帧，最终生成 250 × 4 的 code 矩阵。为了使语言模型能够预测整个音乐片段，我们以逐帧的方式将 4 层的音乐 code 展平为顺序序列。语言模型首先预测第一帧的最初四个标记，并以类似的方式继续预测后续的帧。

### 语言模型

#### 扩展词典

为了将多模态离散的表示合并到预训练的 LLM 中，我们使用新模态的 token 扩展词汇表，从而扩展相应的嵌入和预测层，新合并的嵌入和预测层的参数是随机初始化的。所有模态的 token 结合起来形成一个新的词汇表，其中每种模态都在语言模型中进行训练，目的是在共享的表示空间中对齐。增强词汇量的大小用 $V$ 表示，是所有模态词汇量的总和，即 $V = \sum_{i=1}^n Vi$ ，其中 $V_i$ 表示第 $i$ 个模态的词汇量。

#### 统一多模态

在使用了模态特定的分词器之后，我们可以将多模态数据离散化为 token 序列，因此可以使用语言模型的往后预测的loss来进行训练。这自然让核心 LLM 能够以自回归的方式统一感知、理解、推理和生成等任务。

### 多模态生成

高质量多模态数据（包括高清图像和高保真音频）的生成提出了巨大的挑战。这些数据通常需要大量比特才能准确表示，从而导致序列较长，这对语言模型的要求特别高，因为计算复杂度随着序列长度呈指数级增加。

为了解决这个问题，我们采用了两阶段的高保真生成框架，包括语义信息建模和感知信息建模。首先，语言模型的任务是生成在语义层面经过融合和对齐的内容。然后，非自回归模型在感知层面将多模态语义 token 转换为高保真多模态内容，在性能和效率之间取得平衡。

具体来说，我们使用与扩散模型潜在空间对齐的 SEED token 来进行视觉语言建模。语义级 SEED token 通过扩散模型解码为高质量图像，该模型以其卓越的生成能力而闻名。对于语音，我们使用 SoundStorm 模型，这是一种非自回归掩码语言模型，经过训练可从语义 token 生成 SpeechTokenizer 的声学 token。我们训练了 Soundstorm 的一个变体，它是使用 SpeechTokenizer 在多语言 LibriSpeech(MLS) 数据集上进行训练的。随后，SpeechTokenizer 的解码器将所有语音 token 转换为原始音频数据。这种方法使 AnyGPT 能够使用 3 秒语音提示复制任何说话者的声音，同时显着缩短 LLM 语音序列的长度。对于音乐，我们使用 Encodec token 过滤掉人类感知之外的高频细节，然后使用 Encodec 解码器将这些 token 重建为高保真音频数据。

## 多模态数据

### 预训练数据

为了实现从任何模态到任何其他模态的生成，用有能够良好对齐这些模态的数据集至关重要。不幸的是，这样的数据是严重稀缺的。为了解决这个问题，我们构建了一个以文本为中心、双模态对齐的数据集。文本作为的重要中介来弥合其他模态之间的差距。通过使用不同的模态对齐语言模型中的文本模态，我们希望实现其他模态之间的相互对齐。

![image-20240304132759371](2402.12226%20AnyGPT.assets/pretrain data distribution.png)

预训练数据的分布，按照 token 总数分割。内圈表示模态，中间的环代表具体数据类别，外圈说明具体的数据集。

#### Image & Text

我们利用了 LAION-2B、LAION-COCO、LAION-Aesthetics 和 JouneyDB 中的图像文本对。LAION-2B 提供与来自网络的嘈杂文本配对的图像，而 LAION-COCO 代表其中的 600M 子集，使用 BLIP 为生成标题。我们通过过滤文本质量、图像长宽比和 clip 分数等来细化这些数据集，产生 3 亿对的高质量语料库。为了提高整体图像生成保真度，我们使用高质量的 LAION-Aesthetics 子集和来自 Midjourney 的合成数据集 JourneyDB 来补充我们的数据。

#### Speech & Text

使用几个大型英语自动语音识别（ASR）数据集，总共构成了 57,000 小时的语音文本对的语料库，涵盖了各种各样的说话者、领域和录音环境。

#### Music&Text

我们从互联网上抓取了超过一百万个音乐视频作为数据数据集。核心步骤涉及使用 Spotify API 将这些视频的标题与相应的歌曲进行匹配。随后，我们为每首音乐收集一套全面的元数据，包括视频标题、描述、关键字、播放列表名称和 Spotify 歌词。这些原数据被格式化为json格式，然后喂给 GPT-4。GPT-4 作为一个重要的智能字幕生成器；它利用嘈杂的元数据来提取有意义的信息，并将其简洁地总结为连贯的句子。以上步骤允许我们为大量音乐生成高质量的文本描述，有效的最大限度减少了数据集中的出现幻觉的情况。

#### 构建训练句子

为了训练语言模型 (LM)，我们使用各种模板来构造多模态句子，然后 LM 对其进行自回归处理。更多训练详情请参见附录 A.2。此外，我们观察到不同模态和数据集的句子长度存在显着差异。为了提高训练效率，来自同一数据集的样本被连接成一个长序列，遵循模型的最大序列长度。因此，序列中的每个 token 都会计算损失。

### 多模态交错指导数据集

![image-20240304135634140](2402.12226%20AnyGPT.assets/Multimodal Interleaved Instruction Data Construction.png)

多模态交错指令数据集 AnyInstruct 的构建过程分为两个阶段：生成包含多模态元素的基于文本的对话和文本到多模态的转换。第一阶段生成主题、场景和文本对话，第二阶段生成最终的多模态对话。

#### 生成包含多模态元素的基于文本的对话

在此阶段，我们使用 GPT-4 生成一系列基于文本的对话。值得注意的是，我们在这些对话中以文本描述的形式合并了非文本形态。为了确保获得大规模的高质量数据，我们将此阶段分为三个步骤。

1. 最初，我们头脑风暴了 100 个元主题，以涵盖与视听元素相关的广泛场景，并使用 GPT-4 将这些元主题扩展为 20,000 个特定主题。

2. 随后，我们提示 LLM 根据这些主题生成具体的对话场景。认识到基于文本的 LLM 在生成多模态元素方面的内在限制，我们准备了几个包含尽可能多的模态组合的演示。在生成场景时，将从这些演示中抽取一个样本，作为 LLM 的示例。这种方法指导该模型有效合成多样性和上下文相关的对话场景。

3. 最后，我们利用 GPT-4 生成源自场景的多轮对话。在这些合成的对话中，通过详细的文本形式描绘了多模态元素，包括图像和音乐。我们制作了各种各样的对话示例，类似于场景生成，以提示模型以尽可能使用多样的模态创建对话。最终，我们以单纯文本格式制作了大量的多模态对话数据。

#### 文本到多模态的对话

根据用户的指令和模型的文本响应，使用 DALL-E-3 实现图像生成，使用 MusicGen 生成音乐，使用微软的文本到语音API实现语音生成。

经过过滤之后，我们获得了 108k 高质量多模态对话的数据集，具有多种多模态组合。该数据集包括大约 205k 图像、503k 录音和 113k 音乐曲目。此外，我们从适合口语叙述的现有文本微调数据集中提取对话来增强我们的数据集。然后使用文本转语音模型，可以实现 10 万次语音对话。

## 局限

### 任意模态大模型benchmark

任意模态到任意模态的多模态大语言模型（LLM）领域是一个新兴的研究领域。然而，缺乏专门的基准来评估模型在多个维度的能力以及降低潜在风险，这带来了相当大的挑战，因此，制定一个全面的基准势在必行。

### 增强 LLM

尽管使用离散表示的多模态 LLM 可以稳定地进行训练，但与单模态训练相比，观察到更高的损失，从而阻碍了每种模态的最佳性能。改进多模态融合的潜在策略可能涉及扩展 LLM 和分词器或采用专家混合 (MOE) 架构来更好地管理不同的数据并优化性能。

### 更高的分词器

在采用离散表示的多模态 LLM 中，分词器的质量为模型的理解和生成潜力设定了上限。可以从各个角度增强分词器，包括采用更好的 codebook 训练方法、开发更加综合的多模态表示，以及跨模态的信息解理的应用。

### 更长的上下文

多模态内容（例如图像和音频）通常需要更长的序列。例如，AnyGPT 将音乐建模时间限制为 5 秒，这极大地限制了其音频输出的实际用途。此外，对于任意多模态对话，扩展的上下文允许更多数量的对话交流，从而丰富交互的深度和复杂性。