# 主要内容

## Q-Former

![image-20240219110450305](2301.12597%20BLIP-2.assets/BLIP-2.png)

为了与冻结的单模态模型实现有效的视觉语言对齐，我们提出了一种使用新的两阶段预训练策略进行预训练的查询 Transformer（QFormer）。Q-Former 是一种轻量级转换器，它采用一组可学习的查询向量从冻结图像编码器中提取视觉特征。它充当冻结图像编码器和冻结 LLM 之间的信息瓶颈，为 LLM 提供最有用的视觉特征以输出所需的文本。在第一个预训练阶段，我们进行视觉语言表示学习，强制 Q-Former 学习与文本最相关的视觉表示。在第二个预训练阶段，我们通过将 Q-Former 的输出连接到冻结的 LLM 来执行视觉到语言的生成学习，并训练 Q-Former，使其输出视觉表示可以由 LLM 解释。

![image-20240219112026122](2301.12597%20BLIP-2.assets/Q-Former.png)

（左）Q-Former 和 BLIP-2 第一阶段视觉语言表示学习目标的模型架构。我们联合优化三个目标，强制执行查询（一组可学习的嵌入）以提取与文本最相关的视觉表示。（右）每个目标的自注意力掩蔽策略来控制查询文本交互。

我们提出 Q-Former 作为可训练模块，以弥合冻结图像编码器和冻结 LLM 之间的差距。它从图像编码器中提取固定数量的输出特征，与输入图像分辨率无关。如图所示，Q-Former 由两个共享相同自注意力层的转换器子模块组成：(1) 一个与冻结图像编码器交互以进行视觉特征提取的图像转换器，(2) 一个文本转换器， 可以充当文本编码器和文本解码器。我们创建一组可学习的查询嵌入作为图像转换器的输入。查询通过自注意力层相互交互，并通过交叉注意力层（插入每个其他 Transformer 块）与冻结图像特征交互。查询还可以通过相同的自注意力层与文本进行交互。根据预训练任务，我们应用不同的自注意力掩码来控制查询文本交互。我们使用 BERTbase 的预训练权重来初始化 Q Former，而交叉注意力层是随机初始化的。Q-Former 总共包含 188M 个参数。请注意，查询被视为模型参数。

在我们的实验中，我们使用 32 个查询，其中每个查询的维度为 768（与 Q-Former 的隐藏维度相同）。我们使用 Z 来表示输出查询表示。Z 的尺寸 (32 × 768) 比冻结图像特征的尺寸小得多（例如 ViT-L/14 的尺寸为 257 × 1024）。这种瓶颈架构与我们的预训练目标一起工作，强制查询提取与文本最相关的视觉信息。



## 训练 Q-Former

### stage 1: 视觉语言表示学习 / vision-language representation learning stage

![image-20240219112026122](2301.12597%20BLIP-2.assets/Q-Former.png)

在表示学习阶段，我们将 Q-Former 连接到冻结图像编码器，并使用图像文本对进行预训练。我们的目标是训练 Q-Former，以便查询能够学习提取最能提供文本信息的视觉表示。受 BLIP 的启发，我们联合优化了三个共享相同输入格式和模型参数的预训练目标。每个目标在查询和文本之间采用不同的注意力屏蔽策略来控制它们的交互。

(1) 图像文本对比学习（ITC）学习对齐图像表示和文本表示，以使它们的互信息最大化。它通过对比正对与负对的图像文本相似性来实现这一点。我们将图像转换器的输出查询表示 Z 与文本转换器的文本表示 t 对齐，其中 t 是 [CLS] 标记的输出嵌入。由于 Z 包含多个输出嵌入（每个查询一个），因此我们首先计算每个查询输出与 t 之间的成对相似度，然后选择最高的一个作为图像文本相似度。为了避免信息泄漏，我们采用单模态自注意力掩码，其中查询和文本不允许互相看到。由于使用了冻结图像编码器，与端到端方法相比，我们可以在每个 GPU 上容纳更多样本。因此，我们在 BLIP 中使用批内负数而不是动量队列。*使用上图右侧的mask*。

(2) 以图像为基础的文本生成 (ITG) 损失训练 Q-Former 在给定输入图像作为条件的情况下生成文本。由于 Q-Former 的架构不允许冻结图像编码器和文本标记之间直接交互，因此必须首先通过查询提取生成文本所需的信息，然后通过自注意力层传递给文本标记。因此，查询被迫提取捕获有关文本的所有信息的视觉特征。我们采用多模态因果自注意力掩码来控制查询文本交互，类似于 UniLM 中使用的掩码。查询可以相互关注，但不能关注文本标记。每个文本令牌可以处理所有查询及其之前的文本令牌。我们还将 [CLS] 标记替换为新的 [DEC] 标记，作为指示解码任务的第一个文本标记。*使用上图中间的mask*。

(3) 图像文本匹配（ITM）旨在学习图像和文本表示之间的细粒度对齐。这是一个二分类任务，要求模型预测图像-文本对是正（匹配）还是负（不匹配）。我们使用双向自注意力掩码，所有查询和文本都可以相互关注。因此，输出查询嵌入 Z 捕获多模态信息。我们将每个输出查询嵌入到二类线性分类器中以获得 logit，并将所有查询的 logit 平均作为输出匹配分数。我们采用 Li 等人的硬负挖掘策略。创建信息丰富的负对。*使用上图左边的mask，即没有mask*。

### stage2: 使用冻结的语言模型实现视觉语言生成 / bootstraps vision-to-language generative learning from a frozen language model

![image-20240219130116046](2301.12597%20BLIP-2.assets/Q-Former stage 2.png)

BLIP-2 的第二阶段视觉到语言生成预训练，从冻结的大语言模型 (LLM) 引导。（上）引导基于解码器的 LLM（例如 OPT）。（下）引导基于编码器-解码器的 LLM（例如 FlanT5）。全连接层从 Q-Former 的输出维度适应所选 LLM 的输入维度。

在生成预训练阶段，我们将 Q-Former（附加了冻结图像编码器）连接到冻结的 LLM，以获取 LLM 的生成语言能力。如图 3 所示，我们使用全连接 (FC) 层将输出查询嵌入 Z 线性投影到与 LLM 的文本嵌入相同的维度。然后将投影的查询嵌入添加到输入文本嵌入之前。它们起到软视觉提示的作用，使 LLM 以 Q-Former 提取的视觉表示为条件。由于 Q-Former 已经过预先训练，可以提取包含语言信息的视觉表示，因此它有效地充当信息瓶颈，为 LLM 提供最有用的信息，同时删除不相关的视觉信息。这减轻了 LLM 学习视觉语言对齐的负担，从而减轻了灾难性的遗忘问题。

我们尝试了两种类型的 LLM：基于解码器的 LLM 和基于编码器-解码器的 LLM。对于基于解码器的 LLM，我们使用语言建模损失进行预训练，其中冻结的 LLM 的任务是根据 Q-Former 的视觉表示生成文本。对于基于编码器-解码器的 LLM，我们使用前缀语言建模损失进行预训练，其中我们将文本分为两部分。前缀文本与视觉表示连接起来，作为 LLM 编码器的输入。后缀文本用作 LLM 解码器的生成目标。

### 预训练数据

使用了和 BLIP 相同的 129M 张图片，包括COCO图片。

使用 CapFilt 方法给网络图像创建同步的字幕。具体来说，我们使用 BLIP large 模型生成 10 个字幕，并根据 CLIP ViT-L/14 模型生成的图像文本相似性对合成字幕与原始网页字幕进行排名。我们将每张图像的前两个标题保留为训练数据，并在每个预训练步骤中随机采样一个。

### ViT

移除了ViT的最后一层，使用倒数第二层的输出，可以略微提高性能。

## 实验

### Image Captioning

我们针对图像字幕任务对 BLIP-2 模型进行了微调，该任务要求模型为图像的视觉内容生成文本描述。我们使用提示“a photo of”作为 LLM 的初始输入，并训练模型以生成具有语言建模损失的标题。我们在微调期间保持 LLM 冻结，并与图像编码器一起更新 Q-Former 的参数。

### Visual Question Answering

给定带注释的 VQA 数据，我们微调 Q-Former 和图像编码器的参数，同时保持 LLM 冻结。我们对开放式答案生成损失进行微调，其中 LLM 接收 Q-Former 的输出和问题作为输入，并被要求生成答案。为了提取与问题更相关的图像特征，我们还针对问题设置了 Q-Former 条件。具体来说，问题标记作为 Q-Former 的文本输入，并通过自注意力层与查询交互，这可以引导 Q-Former 的交叉注意力层关注信息更丰富的图像区域。

### Image-Text Retrieval

由于图像文本检索不涉及语言生成，因此我们直接微调第一阶段预训练模型，无需 LLM。具体来说，我们使用与预训练相同的目标（即 ITC、ITM 和 ITG）对图像编码器进行微调，使其与 COCO 上的 Q-Former 一起使用。然后，我们在 COCO 和 Flickr30K 数据集上评估图像到文本检索和文本到图像检索的模型。在推理过程中，我们遵循 Li 等人的观点。首先根据图像文本特征相似度选择 k = 128 个候选者，然后根据成对的 ITM 分数重新排名。我们尝试使用 ViT-L 和 ViT-g 作为图像编码器。详细的超参数可以在附录中找到。

ITC 和 ITM 损失对于图像文本检索至关重要，因为它们直接学习图像文本相似性。我们发现，ITG（基于图像的文本生成）损失也有利于图像文本检索。