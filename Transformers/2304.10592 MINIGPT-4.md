# 主要内容

## 介绍

来自 InstructBLIP 介绍：

> MiniGPT-4 使用与 BLIP-2 相同的预训练视觉编码器和 Q-Former，但使用 Vicuna 作为 LLM，并使用比 BLIP-2 训练数据更多的 ChatGPT 生成的图像标题进行训练 。

我们提出了一种新颖的视觉语言模型，名为 MiniGPT-4。它采用先进的大语言模型 (LLM) Vicuna 作为语言解码器，该模型基于 LLaMA 构建，据报道，根据 GPT-4 的评估，其质量达到了 ChatGPT 90% 的质量。在视觉感知方面，我们采用与 BLIP-2 相同的预训练视觉组件，其中包含来自 EVA-CLIP 的 ViT G/14 和 Q-Former 网络。MiniGPT-4 添加了一个投影层，将编码的视觉特征与 Vicuna 语言模型对齐，并冻结所有其他视觉和语言组件。MiniGPT-4 最初在 4 个 A100 GPU 上使用 256 的批量大小进行 20k 步骤的训练，利用组合图像字幕数据集（包括来自 LAION、概念字幕和 SBU 的图像）将视觉特征与 Vicuna 语言模型对齐。然而，仅仅将视觉特征与语言模型（LLM）结合起来不足以确保强大的视觉对话功能，类似于聊天机器人。原始图像-文本对中潜在噪声的存在可能会导致语言输出低于标准。因此，我们收集了另外 3,500 个详细的图像描述对，通过设计的对话模板进一步微调模型，以提高生成语言的自然度及其可用性。

## 结构

![MiniGPT-4 architecture](2304.10592%20MINIGPT-4.assets/MiniGPT-4%20architecture.png)

训练部分只有一个 Linear

MiniGPT-4 旨在将来自预训练视觉编码器的视觉信息与高级大语言模型 (LLM) 结合起来。具体来说，我们利用 Vicuna 作为我们的语言解码器，它是在 LLaMA的基础上构建的，可以执行各种复杂的语言任务。对于视觉感知，我们采用与 BLIP-2 中使用的相同视觉编码器、ViT 主干网络 以及他们预先训练的 Q-Former。语言和视觉模型都是开源的。我们的目标是使用线性投影层来弥合视觉编码器和 LLM 之间的差距，我们的模型概述如上图所示。

## 训练阶段

为了实现有效的 MiniGPT-4，我们提出了一种两阶段训练方法。初始阶段涉及在大量对齐的图像文本对上对模型进行预训练，以获取视觉语言知识。在第二阶段，我们使用较小但高质量的图像文本数据集和设计的对话模板对预训练模型进行微调，以增强生成的可靠性和可用性。

### 阶段1

在最初的预训练阶段，该模型旨在从大量对齐的图像-文本对中获取视觉-语言知识。我们将注入投影层的输出视为 LLM 的软提示，提示其生成相应的真实文本。

在整个预训练过程中，预训练的视觉编码器和LLM都保持冻结状态，只有线性投影层被预训练。

遇到的问题：在第一个预训练阶段之后，我们的 MiniGPT-4 展示了拥有丰富知识并对人类询问提供合理响应的能力。然而，我们观察到它会产生不连贯的语言输出，例如重复的单词或句子、支离破碎的句子或不相关的内容。这些问题阻碍了 MiniGPT-4 与人类进行流畅的视觉对话的能力。

### 为视觉语言领域策划高质量的对齐数据集。

#### 初步生成对其数据集

初始对齐的图像文本生成在初始阶段，我们采用从第一个预训练阶段导出的模型来生成输入图像的全面描述。为了使我们的模型能够生成更详细的图像描述，我们设计了一个遵循 Vicuna (Chiang et al., 2023) 语言模型的对话格式的提示，如下所示。在此提示中，<ImageFeature> 表示线性投影图层产生的视觉特征。

\###Human: <Img><ImageFeature></Img>Describe this image in detail. Give as many details as possible. Say everything you see. ###Assistant:

为了识别不完整的句子，我们检查生成的句子是否超过 80 个标记。如果没有，我们会添加一个额外的提示，###Human：Continue ###Assistant：，提示我们的 MiniGPT-4 继续生成描述内容。通过拼接两个步骤的输出，我们可以创建更全面的图像描述。这种方法使我们能够生成具有详细且信息丰富的图像描述的图像文本对。我们从 Conceptual Caption 数据集中随机选择 5,000 张图像，并使用预训练模型为每张图像生成相应的语言描述。

#### 数据后处理

数据后处理上述自动生成的图像描述包含噪声或不连贯的描述，例如单词或句子的重复、句子碎片或不相关的内容。为了解决这些问题，我们使用ChatGPT通过以下提示来修复描述：

Fix the error in the given paragraph. Remove any repeating sentences, meaningless characters, not English sentences, and so on. Remove unnecessary repetition. Rewrite any incomplete sentences. Return directly the results witout explanation. Return directly the input paragraph if it is already correct without explanation. / 修正给定段落中的错误。删除任何重复的句子、无意义的字符、非英语句子等。删除不必要的重复。重写所有不完整的句子。直接返回结果，不做任何解释。如果输入段落已经正确，则直接返回，无需解释。

### 阶段2

在第二阶段，我们使用精选的高质量图像文本对来微调我们的预训练模型。在微调过程中，我们使用以下模板中的预定义提示：

###Human: <Img><ImageFeature></Img><Instruction>###Assistant:

在此提示中，<Instruction> 表示从我们的预定义指令集中随机采样的指令，其中包含不同形式的指令，例如“详细描述此图像”或“您能为我描述此图像的内容吗”。值得注意的是，我们不会计算此特定文本图像提示的回归损失。

因此，MiniGPT-4 现在能够生成更自然、更可靠的语言输出。

## 结构消融实验

为了进一步证明使用单个线性层将视觉特征与 LLM 对齐的有效性，我们使用不同的架构设计进行了实验，包括（a）删除 Q Former 并将 VIT 的输出直接映射到 Vicuna 的嵌入空间（即，没有 Q -former），（b）使用三个线性层而不是一层，以及（c）另外微调视觉模块中的 Q-Former。所有变体都以与原始设计相同的方式进行训练。表 4 中 AOK-VQA 和 GQA 数据集的结果表明，没有 Q-Former 的变体 (a) MiniGPT-4 具有与原始设计相似的性能。图 4、图 13 和图 14 中该变体的定性结果也显示了类似的高级技能。这表明 BLIP-2 的 Q-Former 对于高级技能并没有起到关键作用。此外，两种变体 (b) MiniGPT-4+ 3 Layers 和 (c) MiniGPT-4 + 微调 Q Former 的性能都比原始 MiniGPT-4 稍差。这表明单个投影层足以在我们有限的训练数据设置中对齐视觉编码器和大型语言模型。

没有 Q-Former 的变体 (a) MiniGPT-4 具有与原始设计相似的性能。这表明 BLIP-2 的 Q-Former 对于高级技能并没有起到关键作用。（LLaVA直接使用了1层和2层的线性层）
