# 主要内容

## 该模型使用三个视觉语言目标联合预训练

> 图像文本对比学习(Clip，交叉熵损失)、图像文本匹配(预测一个值，是匹配还是不匹配)和图像条件语言建模(生成文字，交叉熵损失)。

![image-20240219092801943](2201.12086%20BLIP.assets/blip structure.png)

BLIP 的预训练模型架构和目标（相同参数具有相同颜色）。我们提出了编码器-解码器的多模态混合，这是一种统一的视觉语言模型，可以在以下三个功能之一中运行：

1. 使用图像文本对比（ITC）损失来训练单模态编码器，以对齐视觉和语言表示。CLIP的方式。属于图文理解。
2. 基于图像的文本编码器使用额外的 Cross Attention 来建模视觉-语言交互，并使用图像-文本匹配（ITM）损失进行训练，以区分正负图像-文本对。添加特殊的 [Encoder] token 添加到text前面，用作图文对的多模态表示，用来表示图片是否匹配。属于图文理解。
3. 基于图像的文本解码器用 Causal Self-Att(带掩码) 替换了 Bi Self-Att，并与编码器共享相同的 Cross Attention 和 FFN。解码器经过语言建模 (LM) 损失的训练，以生成给定图像的标题。属于图文生成。

视觉模型使用了ViT，并添加了 [CLS] token 来作为图片的全局表示

单模态编码器，分别对图像和文本进行编码。文本编码器与 BERT 相同，其中 [CLS] 标记附加到文本输入的开头以总结句子。

基于图像的文本编码器，通过在文本编码器的每个 Transformer 块的自注意 (SA) 层和前馈网络 (FFN) 之间插入一个额外的交叉注意 (CA) 层来注入视觉信息。特定于任务的 [Encode] 标记附加到文本中，[Encode] 的输出嵌入用作图像-文本对的多模态表示。

基于图像的文本解码器，用 Causal Self-Att(带掩码) 替换基于图像的文本编码器中的 Bi Self-Att。[Decode] 标记用于表示序列的开始，序列结束标记用于表示序列的结束。

为了在利用多任务学习的同时执行高效的预训练，文本编码器和文本解码器共享除 SA 层之外的所有参数。原因是 SA 层最好地捕获编码和解码任务之间的差异。特别是，编码器采用双 bi self-attention 来构建当前输入令牌的表示，而解码器则采用 casual self-attention 来预测下一个令牌。另一方面，嵌入层、CA层和FFN在编码和解码任务之间的功能相似，因此共享这些层可以提高训练效率，同时受益于多任务学习。

## 字幕和过滤（CapFilt）

>  一种新的数据集 boostrapping 捕获方法，用于从噪声图像文本对中学习。我们将预训练的 MED 微调为两个模块：一个用于根据给定网络图像生成合成字幕的字幕器，以及一个用于生成合成字幕的过滤器。从原始网络文本和合成文本中删除嘈杂的标题。

![image-20240219094232337](2201.12086%20BLIP.assets/CapFilt.png)

BLIP的学习框架。我们引入了一个字幕生成器来为网络图像生成合成字幕，并引入一个过滤器来去除嘈杂的图像文本对。字幕生成器和过滤器是从相同的预训练模型初始化的，并在小型人工注释数据集上单独进行微调。引导数据集用于预训练新模型。

我们提出了字幕和过滤（CapFilt），这是一种提高文本语料库质量的新方法。图 3 给出了 CapFilt 的图示。它引入了两个模块：一个用于在给定网络图像的情况下生成字幕的字幕生成器，以及一个用于消除噪声图像文本对的过滤器。字幕生成器和过滤器都是从相同的预训练 MED 模型初始化的，并在 COCO 数据集上单独进行微调。微调是一个轻量级的过程。

具体来说，字幕生成器是一个基于图像的文本解码器（第二个模型）。它通过 LM 目标进行微调，以解码给定图像的文本。给定网络图像 Iw，字幕生成器生成合成字幕 Ts，每个图像一个字幕。

该过滤器是一个基于图像的文本编码器（第三个模型）。它根据 ITC 和 ITM 目标进行了微调，以了解文本是否与图像匹配。该过滤器会去除原始网络文本 Tw 和合成文本 Ts 中的噪声文本，其中如果 ITM 头预测文本与图像不匹配，则文本被认为是噪声文本。最后，我们将过滤后的图像文本对与人工注释对结合起来形成一个新的数据集，我们用它来预训练新模型。

## 针对不同任务的调整

### Visual Question Answering (VQA)

![image-20240219100544789](2201.12086%20BLIP.assets/VQA.png)

我们重新安排了预训练模型，其中图像问题首先被编码为多模态嵌入，然后提供给答案解码器。VQA 模型使用真实答案作为目标，通过 LM 损失进行微调。

### Natural Language Visual Reasoning (NLVR^2)

![image-20240219100856467](2201.12086%20BLIP.assets/image-20240219100856467.png)

NLVR2 要求模型预测一个句子是否描述了一对图像。

对于基于图像的文本编码器中的每个 Transformer 块，存在两个交叉注意层来处理两个输入图像，并且它们的输出被合并并馈送到 FFN。两个 CA 层由相同的预训练权重初始化。合并层在编码器的前 6 层中执行简单平均池化，并在第 6-12 层中执行 concat，然后执行线性投影。MLP 分类器应用于 [Encode] token 的输出嵌入。

### VisDial

![image-20240219100917750](2201.12086%20BLIP.assets/image-20240219100917750.png)

DisDial 在自然对话环境中扩展了 VQA，其中模型不仅需要基于图像-问题对来预测答案，还需要考虑对话历史记录和图像的标题。我们遵循判别性设置，模型对候选答案池进行排名。如图所示，我们连接 image 和 text 嵌入，并通过交叉注意力将它们传递给对话编码器。对话编码器使用 ITM 损失进行训练，以在给定整个对话历史记录和图像标题嵌入的情况下区分问题的答案是真还是假。

