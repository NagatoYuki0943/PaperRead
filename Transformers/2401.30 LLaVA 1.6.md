https://llava-vl.github.io/blog/2024-01-30-llava-next/

# 主要

## 1.6相比1.5的改进

与LLaVA-1.5相比，LLaVA-NeXT有几点改进：

1. **将输入图像分辨率提高到** 4 倍以上的像素。这使它能够掌握更多的视觉细节。它支持三种纵横比，最高分辨率为 672x672、336x1344、1344x336。CLIP vision encoders的最高分辨率为336，672和1344是336的2倍和4倍。
2. **更好的视觉推理和 OCR 功能**，以及改进的视觉指令调整数据组合。
3. **更好的可视化对话，适用于更多场景，**涵盖不同的应用程序。更好的世界知识和逻辑推理。
4. 使用 [SGLang](https://github.com/sgl-project/sglang) 进行**高效部署和推理**。

除了性能改进外，**LLaVA-NeXT 还保持了 LLaVA-1.5 的极简设计和数据效率**。它重用了 LLaVA-1.5 的预训练连接器，仍然使用不到 1M 的视觉指令调优样本。最大的 34B 变体在 ~1 天内完成训练，配备 32 架 A100。**代码、数据、模型将公开提供。**

## 详细的技术改进

### （1） 动态高分辨率

我们以高分辨率设计模型，以保持**其数据效率**。当提供高分辨率图像和保留这些细节的表示时，模型感知图像中复杂细节的能力会显着提高。它减少了模型幻觉，当面对低分辨率图像时，这种幻觉会推测想象的视觉内容。我们的“AnyRes”技术旨在适应各种高分辨率的图像。我们采用的网格配置为 `{2×2,1×{2,3,4}，{2,3,4}×1}`，平衡性能效率和运营成本

![img](240130%20LLaVA%201.6.assets/high_res_arch_v2.png)

*动态高分辨率方案示意图：网格配置2×2*

> 使用 224 * 224 分辨率作为例子
>
> 分割后的patch的feature会被合并回单个大特征图，然后扁平化，最后拼接resize的全局特征图的扁平化数据。

我们使用 CLIP-ViT-L-14 (224 * 224) 作为基础图像编码器。我们首先选择输入图像并将其填充到能够有效捕获其细节的目标分辨率，并将图像分割为 224 * 224 网格。所有 224 * 224 图像块均由 CLIP 图像编码器单独编码，并且它们的特征被合并回单个大特征图。然后，我们将生成的特征图后期处理为扁平化的特征列表。我们还拼接了固定分辨率图像的特征，为模型提供全局上下文。

### （2） 数据混合

- **高质量的用户指令数据**。我们对高质量可视化指令遵循数据的定义取决于两个主要标准：首先，任务指令的多样性，确保充分代表在实际场景中可能遇到的广泛的用户意图，特别是在模型的部署阶段。其次，响应的优越性至关重要，目的是征求用户的积极反馈。为了实现这一点，我们考虑了两个数据源：（1）现有的GPT-V数据。[LAION-GPT-V](https://huggingface.co/datasets/laion/gpt4v-dataset) 和 [ShareGPT-4V](https://sharegpt4v.github.io//)。（2）为了进一步促进更多场景的视觉对话，我们收集了一个涵盖不同应用的小型15K视觉指令调优数据集。说明和图像来自 [LLaVA 演示](https://llava-vl.github.io/)，这是真实用户的要求。我们仔细过滤可能存在隐私问题或可能有害的样本，并使用 GPT-4V 生成响应。
- **多模态文档/图表数据**。（1） 我们从训练数据中删除 TextCaps，因为我们意识到 [TextCaps](https://textvqa.org/textcaps/) 使用与 [TextVQA](https://textvqa.org/) 相同的训练图像集。这使我们能够在开发过程中评估 TextVQA 时更好地了解模型的零样本 OCR 功能。为了保持并进一步提高模型的 OCR 功能，我们将 TextCaps 替换为 DocVQA 和 SynDog-EN。（2） 在 [Qwen-VL-7B-Chat](https://huggingface.co/Qwen/Qwen-VL) 的启发下，我们进一步添加了 ChartQA、DVQA 和 AI2D，以便更好地理解图表和图表。

### （3） 扩展 LLM 骨干网

除了 Vicuna-1.5（7B 和 [13B](https://huggingface.co/lmsys/vicuna-13b-v1.5)）之外，我们还考虑了更多的 LLM，包括 [Mistral-7B](https://mistral.ai/news/announcing-mistral-7b/) 和 [Nous-Hermes-2-Yi-34B](https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B)。这些 LLM 具有良好的属性、灵活的商业使用条款、强大的双语支持和更大的语言模型容量。它允许 LLaVA 支持社区中更广泛的用户和更多场景。LLaVA 配方适用于各种 LLM，并且随着 LLM 的顺利扩展，最高可达 34B。
