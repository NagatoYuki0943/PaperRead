# 主要内容

## 使用gpt4创建多模态指导数据集

社区见证了从 CC 到 LAION 的图像文本对等公共多模态数据量的激增。然而，当涉及到多模态指令跟踪数据时，可用的数量是有限的，部分原因是当考虑到人类群体搜索时，该过程非常耗时且定义不明确。受到最近 GPT 模型在文本注释任务中成功的启发，我们建议利用 ChatGPT/GPT-4 基于广泛存在的图像对数据进行多模式指令跟踪数据收集。

对于图像 Xv 及其关联的标题 Xc，很自然地创建一组问题 Xq ，旨在指导助手描述图像内容。我们提示 GPT-4 并在附录的表 8 中整理了这样一个问题列表。因此，将图像文本对扩展为其指令跟踪版本的简单方法是 Human : Xq Xv<STOP>\n Assistant : Xc<STOP>\n。尽管构建成本低廉，但这个简单的扩展版本在指令和响应中缺乏多样性和深入的推理。

为了缓解这个问题，我们利用纯语言 GPT-4 或 ChatGPT 作为强大的老师（两者都只接受文本作为输入），来创建涉及视觉内容的指令跟踪数据。具体来说，为了将图像编码为其视觉特征以提示纯文本 GPT，我们使用两种类型的符号表示：（i）字幕通常从不同角度描述视觉场景。(ii) 边界框通常定位场景中的对象，每个框对对象概念及其空间位置进行编码。表 1 的顶部块显示了一个示例。

这种符号表示允许我们将图像编码为 LLM 可识别的序列。我们使用 COCO 图像并生成三种类型的指令跟随数据。表 1 的底部块显示了每种类型的一个示例。对于每种类型，我们首先手动设计一些示例。它们是我们在数据收集过程中拥有的唯一人工注释，并用作上下文学习中的种子示例来查询 GPT-4。

- 对话。我们设计了助理和询问有关这张照片问题的人之间的对话。回答的语气就好像助理看到图像并回答问题一样。针对图像的视觉内容提出一系列不同的问题，包括对象类型、对象计数、对象动作、对象位置、对象之间的相对位置。仅考虑有明确答案的问题。详细提示见表10。
- 详细说明。为了对图像进行丰富而全面的描述，我们出于这样的目的创建了一个问题列表。我们提示 GPT-4，然后整理列表，如附录中的表 9 所示。对于每张图像，我们从列表中随机抽取一个问题，要求 GPT-4 生成详细描述。
- 复杂的推理。以上两种类型侧重于视觉内容本身，在此基础上我们进一步创建深度推理问题。答案通常需要遵循严格的逻辑进行逐步推理过程。

我们总共收集了 158K 个独特的语言图像指令跟踪样本，其中对话样本 58K，详细描述样本 23K，复杂推理样本 77K。我们在早期实验中取消了 ChatGPT 和 GPT-4 的使用，发现 GPT-4 可以始终如一地提供更高质量的指令跟踪数据，例如空间推理。

## 模型结构

![image-20240220152018846](2304.08485%20LLaVA.assets/llava architecture.png)

图片使用 CLIP 的 ViT-L/14 模型得到图片特征，语言模型使用 LLaMA，使用一个 Linear 将CLIP的输出投影到语言模型的维度，将经过投影的输出拼接语言指令一同放入语言模型中。

使用CLIP的 Vit-L/14 作为图片的encoder，使用最后一个 Transformer 层之前和之后的网格特征。我们考虑一个简单的线性层将图像特征连接到词嵌入空间。具体来说，应用可训练的投影矩阵 W 将 Zv 转换为语言嵌入标记 Hq，它们与语言模型中的词嵌入空间具有相同的维度。

## 训练

### stage1: 对齐视觉模型输出到LLM

使用单轮对话微调，冻结 ViT 和 LLaMA，只训练 Linear，将 ViT 的输出对齐到 LLM 的  word embedding。使用过滤的 CC3M 数据 595K 个图像-文本对进行微调。



### stage2: 对话微调

仍然冻结 ViT，训练 Linear 和 LLaMA，使用COCO数据集和Science QA数据集微调。

- 多模式聊天机器人。我们通过对第 3 节中收集的 158K 独特语言图像指令遵循数据进行微调来开发聊天机器人。在三种类型的响应中，对话是多轮的，而其他两种是单轮的。它们在训练中被统一采样。

- 科学质量保证。我们在 ScienceQA 基准上研究我们的方法，这是第一个大规模多模式科学问题数据集，用详细的讲座和解释来注释答案。每个问题都以自然语言或图像的形式提供上下文。助手以自然语言提供推理过程，并从多个选项中选择答案。对于 (2) 中的训练，我们将数据组织为单轮对话，将问题和上下文组织为 Xinstruct，将推理和答案组织为 Xa。