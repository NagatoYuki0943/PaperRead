# RAFT

在本文中，我们提出了检索增强微调 （RAFT），这是一种训练方法，可提高模型在“开卷”域内设置中回答问题的能力。在 RAFT 中，给定一个问题和一组检索到的文档，我们训练模型忽略那些无助于回答问题的文档，我们称之为干扰文档。RAFT 通过逐字引用相关文档中的正确顺序来帮助回答问题来实现这一点。这与RAFT的思维链风格响应相结合，有助于提高模型的推理能力。

## 当前方法局限性

在使 LLM 适应专业领域时，我们考虑了以下两种方式：通过检索增强生成 （RAG） 和监督微调进行上下文学习。 基于 RAG 的方法允许 LLM 在回答问题时引用文档。 然而，这些方法未能利用固定领域的设置和早期访问测试文档提供的学习机会。 另一方面，监督微调提供了学习文档中的更多常规模式的机会，并更好地与最终任务和用户偏好保持一致. 然而，现有的基于微调的方法要么无法在测试时利用文档（不合并 RAG），要么无法解决训练期间检索过程中的缺陷。

![](2403.10131v1%20RAFT.assets/1.png)

如何最好地准备考试？（a） 基于微调的方法通过直接“记忆”输入文件或回答实践质量保证而不参考文件来实现“学习”。（b） 或者，上下文检索方法未能利用固定领域提供的学习机会，相当于不学习就参加开卷考试。虽然这些方法利用了领域内学习，但它们未能为开卷测试做好准备。相比之下，我们的方法 （c） RAFT 利用问答对进行微调，同时在模拟的不完美检索环境中引用文档，从而有效地为开卷考试设置做准备。

## 详解

在本文中，我们研究了如何将监督微调（SFT）与检索增强生成（RAG）相结合。 我们提出了一种新颖的适应策略——检索增强微调（RAFT）。 RAFT 专门解决了微调 LLM 以整合领域知识的挑战，同时提高了域内 RAG 性能。 RAFT不仅旨在使模型能够通过微调来学习特定领域的知识，而且还要确保针对不准确检索的鲁棒性。 这是通过训练模型来理解提出的问题（提示）、检索到的领域特定文档和适当的答案之间的动态关系来实现的。 回到我们的类比，我们的方法类似于通过识别相关和不相关的检索文档来学习开卷考试。

![](2403.10131v1%20RAFT.assets/2.png)

RAFT 方法概述。 左上图描述了我们使 LLM 适应一组正面和负面文档的阅读解决方案的方法，这与标准 RAG 设置形成鲜明对比，在标准 RAG 设置中，模型基于检索器输出进行训练，这是记忆和阅读的混合体。在测试时，所有方法都遵循标准的 RAG 设置，并在上下文中提供 top-k 检索到的文档。

### 检索文档中添加干扰文档

检索感知微调 （RAFT） 提出了一种新的方法，用于准备微调数据，以针对特定领域的开卷设置定制模型，相当于在领域内使用 RAFT 的 RAG，在准备训练数据时，每个数据包含一个问题（𝑄）、一组文档 （$𝐷_𝑘$），以及根据其中一个文档（$𝐷^*$) 生成的 Chain-of-though 样式答案 （$A^*$）。我们将文档分为两种类型：Oracle 文档，即可以从中推断出问题答案的文件，以及“干扰”文件，即不包含与答案相关的信息。“oracle”文档不仅是单个文档，还可以是多个文档，就像 HotpotQA 中的情况一样。

### 仅使用干扰文档训练

对于数据集中的一部分 $\frac P 1$ 问题（$𝑞_i$） ，我们保留了 Oracle 文档 （$𝑑_i^*$） 以及干扰文档 （$𝑑_{𝑘−1}$)。在剩余$\frac {1−𝑃} 1$ 的问题（$𝑞_i$），我们不包含 Oracle 文档，只包含干扰文档（$𝑑_𝑘$​​)。然后使用 SFT 训练方式让模型根据上面的问题和文档生成回答。


$$
P \ \% \ of \ data: Q + D^∗ + D_2 + . . . + D_k → A^∗ 
\\
(1 − P) \ \% \ of \ data: Q + D_1 + D_2 + . . . + D_k → A^∗
$$


在某些情况下，通过删除"Oracle"文档，我们迫使模型记住答案，而不是从上下文中检索答案。

### 使用 CoT

提高训练质量的一个关键因素是生成推理的过程，例如思维链，用来解释所提供的答案。RAFT方法与此类似：我们发现创建一个完整的推理链和清楚地引用来源可以提高模型回答问题的准确性。

![](2403.10131v1%20RAFT.assets/3.png)

RAFT 提示帮助 LLM 评估自己生成的推理和答案，将它们与正确的推理和答案进行对比。LLM 被提示识别其推理中的错误并提取改进的关键见解。

## 评估

### 基本能力

总体而言，LLaMA-7B模型，无论是否使用RAG，都表现不佳，因为它的应答风格与真实答案不一致。通过应用特定领域的调整，我们显著提高了其性能。此过程使模型能够学习并采用适当的回答风格。但是，将 RAG 引入特定领域微调 （DSF） 模型并不一定能带来更好的结果。这可能表明该模型缺乏上下文处理和从中提取有用信息的能力。通过结合 RAFT ，我们训练模型不仅能使其应答风格与所需的风格相匹配，还能提高其文档处理能力。

### CoT

仅仅提供问题的答案可能并不总是足够的。这种方法可以导致损失的快速减少，从而导致训练过程发散。结合一个推理链，不仅可以引导模型找到答案，还可以丰富模型的理解，可以提高整体准确性。推理链是通过 GPT4 来生成的。

### RAFT 相比 DSF 更有优势

为了说明 RAFT 相对于特定领域微调 (DSF) 方法的潜在优势，我们在下图中提供了一个比较示例。该示例定性地演示了 DSF 模型因询问一个编剧的身份的问题而感到困惑的场景。 它没有提供正确的名称，而是错误地引用了编剧写的一部电影。 相比之下，RAFT 模型准确地回答了这个问题。 这种差异表明，仅使用问答对训练模型可能会削弱其从提供的文档中获取相关上下文的能力。该比较展示了将标准指令微调和上下文理解纳入训练数据集以保留和增强模型有效处理文本的能力的重要性。

![4](2403.10131v1%20RAFT.assets/4.png)

### 检索文档中应该保留多少有效信息

在探讨大型语言模型（LLM）是否应始终使用检索增强生成（RAG）的 Oracle 上下文进行训练时，我们提出了一个关键问题：训练数据中应包含 Oracle 文档的比例 （p%）是多少？人们直观地的认为，为了有效地从上下文中读取和提取信息（例如 RAG 任务），在训练期间应始终包含 Oracle 文档 （P = 100%）。然而，我们的研究结果挑战了这一假设：在上下文中合并一部分没有 Oracle 文档的训练数据（P = 80%）似乎可以提高模型在RAG任务上的性能。

下图展示了我们对超参数 P% 的测试，它表示应包含 Oracle 文档的训练实例的百分比。我们的分析表明，最佳比例因数据集而异，数字从 40%、60% 和 100% 不等。这表明，有时在没有正确对应上下文的情况下训练 LLM 对于回答与文档相关的问题的下游任务是有益的。在我们的训练设置中，我们为 oracle 文档提供四个干扰文档，在测试时，我们通过为  Oracle  文档提供四个干扰文档来保持这种格式。我们的研究结果表明，对于特定领域的 RAG 任务，在上下文中包含一定比例的训练数据而没有 Oracle 文档被证明是有利的。

![](2403.10131v1%20RAFT.assets/5.png)

我们研究超参数 𝑃%，它标明了数据集中包含 Oracle 文档的数据比例。NQ、TQA 和 HotpotQA 的结果表明，一小部分数据中的上下文中没有 Oracle 文档的有助于域内 RAG。

## RAFT 推广到 Top-K RAG

在评估期间使用 top-k 检索器增强生成 （RAG） 结果增强时，RAFT 中的干扰文档数量如何影响模型的性能？ 以前的研究强调了LLM对不相关文本的脆弱性。这个问题对于 LLMs + RAG 尤为重要，因为在测试时经常使用 top-k RAG 来确保高召回率。 这种情况要求模型能够识别和忽略不相关的内容，只关注相关信息。

### 增强 top-K RAG 的健壮性

为了应对增强大型语言模型（LLM）在检索管道中筛选不相关文本的能力的挑战，我们的分析表明，仅使用 oracle（高度相关）文档进行训练可能会无意中削弱模型识别和忽略不相关信息的能力。为了解决这个问题，我们的算法 RAFT 采用了一种策略，将 Oracle 文档与不相关的文档混合在一起。这种方法促使我们调查负面（不相关）文档的理想比例，以纳入整个训练过程，并评估这种训练方法对检索增强生成 （RAG） 在测试阶段遇到的不同文档量的适应程度。我们的目标是完善相关和不相关信息之间的平衡，以加强模型在识别和利用相关内容方面的效率。而在本节中，我们研究了测试时场景。

**使用不相关文档进行训练** 为了增强大型语言模型 （LLM） 对检索到的文档中不相关文本的健壮性，我们采用了一种微调方法，该方法同时使用高度相关文档和干扰（不相关）文档。模型使用不同数量的干扰文档进行训练，但始终使用从检索器获得的前 k 个文档进行评估。

下图展示了包含干扰文档的配置相比，仅使用 Oracle 文档进行微调通常会导致性能较差。从图中可以看出，Natural Questions 使用$𝐷^*+3⁢𝐷$ 可以获得更好性能，Hotpot QA 使用 $𝐷^*+1⁢𝐷$ 可以获取更好的性能。 在我们的实验中，我们通常采用由一个 Oracle 文档和四个干扰文档组成的训练设置。这种方法取得了平衡，确保模型不会被干扰因素淹没，同时仍然能够有效地辨别相关信息并确定其优先级。

![](2403.10131v1%20RAFT.assets/6.png)

我们研究了 RAFT 对检索器可能提供的不同数量的文档的鲁棒性。在 NQ 中，我们发现使用 4 个文档进行训练可以获得最佳性能，但在 HotpotQA 使用 2 个文档进行训练是最佳选择。在这两个数据集中，仅使用由 Oracle 文档组成的数据集进行训练会损害性能。

**推广到可变数量的测试时文档** 我们扩展了我们的研究，以检查不同数量的测试时文档对模型性能的影响。 具体来说，我们的实验重点是评估使用不同数量的干扰文档进行训练的模型如何响应测试时呈现的文档数量的变化。

如上图所示。在训练过程中包含干扰项文档确实使模型更能适应测试期间遇到的文档数量波动。尽管测试时文档数量存在变化，但仍能保持一致的性能，这进一步验证了我们的 RAFT 方法的稳健性。这一发现强调了一个经过良好校准的训练环境的重要性，以便为模型在实际应用中可能遇到的一系列场景做好准备。

## 总结

RAFT 是一种训练策略，旨在提高模型在“开卷”设置中回答特定领域内问题的表现。此技术演示了基于选定文档集合的问答任务的 LLM 微调方法。我们已经确定了几个关键的设计：将相关文档与干扰文档一起训练；调整数据集，使一部分数据集的上下文中缺少相关文档，仅有干扰文档；使用思维链的方式生成答案，并直接引用相关文本。

