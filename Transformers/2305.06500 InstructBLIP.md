# 主要内容

## 相关工作

通过向 LLM 注入视觉信息，指令调整的 LLM 已适应视觉到语言生成任务。BLIP-2 使用冻结的 FlanT5 模型，并训练 Q-Former 来提取视觉特征作为 LLM 的输入。MiniGPT-4 使用与 BLIP-2 相同的预训练视觉编码器和 Q-Former，但使用 Vicuna 作为 LLM，并使用比 BLIP-2 训练数据更多的 ChatGPT 生成的图像标题进行训练 。LLaVA 直接将视觉编码器的输出投影为 LLaMA/Vinuca LLM 的输入，并根据 GPT-4  生成的视觉语言对话数据对 LLM 进行微调。mPLUG-owl 使用来自 LLaVA 的文本指令数据和视觉语言指令数据对 LLaMA 模型执行低秩自适应。一项单独的工作是 MultiInstruct，它在没有预先训练的 LLM 的情况下执行视觉语言指令调整，导致性能竞争力下降。

## vision-language task approach way

与 NLP 任务相比，由于来自各个领域的额外视觉输入，视觉语言任务本质上更加多样化。这对统一模型提出了更大的挑战，该模型应该泛化到不同的视觉语言任务，其中许多任务在训练期间是看不见的。大多数以前的工作可以分为两种方法。第一种方法是多任务学习(multitask learning)，将各种视觉语言任务制定为相同的输入输出格式。然而，我们凭经验发现，没有指令的多任务学习（表 4）不能很好地推广到未见过的数据集和任务。第二种方法使用额外的视觉组件扩展了预训练的 LLM，并使用图像标题数据训练视觉组件。然而，这些数据太有限，无法广泛推广到需要视觉描述之外的视觉语言任务。

## 调整 BLIP-2

InstructBLIP 使用不同的指令数据集来训练多模态 LLM。具体来说，我们使用预训练的 BLIP-2 模型来初始化训练，该模型由图像编码器、LLM 和查询转换器 (Q-Former) 来桥接两者。在指令调整期间，我们对 Q-Former 进行微调，同时保持图像编码器和 LLM 冻结。

(1) 我们对视觉语言指令调整进行了全面、系统的研究。我们将 26 个数据集转换为指令调整格式，并将它们分为 11 个任务类别。我们使用 13 个保留数据集进行指令调整，使用 13 个保留数据集进行零样本评估。此外，我们保留了四个完整的任务类别用于任务级别的零样本评估。详尽的定量和定性结果证明了 InstructBLIP 在视觉语言零样本泛化方面的有效性。

(2) 我们提出了指令感知视觉特征提取，这是一种新颖的机制，可以根据给定的指令进行灵活且信息丰富的特征提取。具体来说，文本指令不仅提供给冻结的 LLM，还提供给 Q-Former，以便它可以从冻结的图像编码器中提取指令感知的视觉特征。此外，我们提出了一种平衡采样策略来同步跨数据集的学习进度。

(3) 我们使用两个 LLM 系列评估并开源了一套 InstructBLIP 模型：1）FlanT5 [7]，一个从 T5 [34] 微调的编码器-解码器 LLM； 2) Vicuna [2]，从 LLaMA [41] 微调的仅解码器的 LLM。InstructBLIP 模型在各种视觉语言任务上实现了最先进的零样本性能。此外，当用作单个下游任务的模型初始化时，InstructBLIP 模型可实现最先进的微调性能。

## Vision-Language Instruction Tuning

### Tasks and Datasets

收集了11个类别的26个数据集。

对于每项任务，我们都会用自然语言精心制作 10 到 15 个不同的指令模板。这些模板作为构建指令调整数据的基础，阐明了任务和目标。对于本质上偏向于短响应的公共数据集，我们在一些相应的指令模板中使用诸如“短”和“简要”之类的术语，以降低模型过度拟合而始终生成短输出的风险。对于 LLaVA-Instruct-150K 数据集，我们没有合并额外的指令模板，因为它自然地以指令格式构建。

数据集分为训练和不训练的2类，每一类13个数据集，有的类别的数据一部分拿来训练另一部分拿来测试，而有的类别全部拿来训练或测试。

### Instruction-aware Visual Feature Extraction 模型结构

![image-20240219164218987](2305.06500%20InstructBLIP.assets/InstructBLIP architecture.png)

InstructBLIP 的模型架构。Q-Former 从冻结图像编码器的输出嵌入中提取指令感知的视觉特征，并将视觉特征作为软提示输入提供给冻结的 LLM。我们使用语言建模损失对模型进行指令调整以生成响应。

我们在上图中展示了 InstructBLIP 的架构。与 BLIP-2 类似，InstructBLIP 利用查询转换器或 Q-Former 从冻结图像编码器中提取视觉特征。Q-Former 的输入包含一组 K 个可学习的查询嵌入，它们通过交叉注意力与图像编码器的输出交互。Q-Former 的输出由 K 个编码视觉向量组成，每个查询嵌入一个，然后经过线性投影并馈送到冻结的 LLM。与 BLIP-2 一样，Q-Former 在指令调整之前使用图像标题数据分两个阶段进行预训练。第一阶段使用冻结图像编码器对 Q-Former 进行预训练，以进行视觉语言表示学习。第二阶段采用 Q-Former 的输出作为软视觉提示，用于使用冻结的 LLM 生成文本。预训练后，我们通过指令调整对 Q-Former 进行微调，其中 LLM 接收来自 Q-Former 的视觉编码和任务指令作为输入。

InstructBLIP 扩展了 BLIP-2，提出了一个指令感知 Q-former 模块，该模块将指令文本标记作为附加输入。该指令通过 Q-Former 的自注意力层与查询嵌入进行交互，并鼓励提取与任务相关的图像特征。因此，LLM 收到有利于遵循指令的视觉信息。我们凭经验证明（表 2），指令感知视觉特征提取为保留和保留评估提供了显着的性能改进。