# 主要内容

## 结构

![architecture](2303.05499%20Grounding%20DINO.assets/Grounding DINO architecture.png)

Grounding DINO 是双编码器单解码器架构。它包含用于图像特征提取的图像主干、用于文本特征提取的文本主干、用于图像和文本特征融合的特征增强器（[Feature Extraction and Enhancer](# Feature Extraction and Enhancer)）、用于查询初始化的语言引导查询选择模块（[Language-Guided Query Selection](# Language-Guided Query Selection)），以及 用于框细化的跨模态解码器（[Cross-Modality Decoder](#  Cross-Modality Decoder)）。整体框架如上图所示。

对于每个（图像，文本）对，我们首先分别使用图像主干和文本主干提取普通图像特征和普通文本特征。这两个普通特征被输入到特征增强器模块中以进行跨模态特征融合。获得跨模态文本和图像特征后，我们使用语言引导的查询选择模块从图像特征中选择跨模态查询。与大多数 DETR 类模型中的对象查询一样，这些跨模态查询将被输入跨模态解码器，以从两个模态特征中探测所需的特征并更新自身。最后一个解码器层的输出查询将用于预测对象框并提取相应的短语。

### Feature Extraction and Enhancer

给定一个（图像，文本）对，我们使用 Swin Transformer 等图像主干提取多尺度图像特征，并使用 BERT 等文本主干提取文本特征。和之前类似 DETR 的检测器一样，从不同块的输出中提取多尺度特征。提取普通图像和文本特征后，我们将它们输入特征增强器中以进行跨模态特征融合。特征增强器包括多个特征增强器层。我们在上图 block 2 中展示了特征增强器层。我们利用可变形自注意力来增强图像特征，并利用普通自注意力来增强文本特征增强器。受 GLIP 的启发，我们添加了图像到文本交叉注意和文本到图像交叉注意以进行特征融合。这些模块有助于协调不同模式的特征。

### Language-Guided Query Selection

```python
"""
Input:
image_features: (bs, num_img_tokens, ndim)
text_features: (bs, num_text_tokens, ndim)
num_query: int.
Output:
topk_proposals_idx: (bs, num_query)
"""
logits = torch.einsum("bic,btc->bit", image_features, text_features)
# bs, num_img_tokens, num_text_tokens
logits_per_img_feat = logits.max(-1)[0]
# bs, num_img_tokens
topk_proposals_idx = torch.topk(logits_per_image_feature, num_query, dim=1)[1]
# bs, num_query
```

Grounding DINO 旨在从输入文本指定的图像中检测对象。为了有效地利用输入文本来指导对象检测，我们设计了一个语言引导的查询选择模块，以选择与输入文本更相关的特征作为解码器查询。我们以 PyTorch 风格展示查询选择过程（上面代码）。变量 image_eatures 和 text_features 分别用于图像和文本特征。num_query 是解码器中的查询数量，在我们的实现中设置为 900。我们在伪代码中使用 bs 和 ndim 作为批量大小和特征维度。num_img_tokens 和 num_text _tokens 分别用于图像和文本标记的数量。

语言引导查询选择模块输出 num 个查询索引。我们可以根据所选索引提取特征来初始化查询。和 DINO 相同，我们使用混合查询选择来初始化解码器查询。每个解码器查询包含两部分：分别是内容部分和位置部分。我们将位置部分表示为动态锚框，其使用编码器输出进行初始化。另一部分，即内容查询，被设置为在训练期间可学习。

### Cross-Modality Decoder

我们开发了一个跨模态解码器来组合图像和文本模态特征，如上图 block 3 所示。每个跨模态查询都被送入到自注意力层、图像交叉注意力层以组合图像特征、文本交叉注意力层结合文本特征，每个跨模态解码器层都有一个 FFN 层。与 DINO 解码器层相比，每个解码器层都有一个额外的文本交叉注意层，因为我们需要将文本信息注入到查询中以获得更好的模态对齐。

###  Sub-Sentence Level Text Feature

![Sub-Sentence Level Text Feature](2303.05499%20Grounding%20DINO.assets/Sub-Sentence%20Level%20Text%20Feature.png)

之前的工作探索了两种文本提示，我们将其命名为句子级表示和单词级表示，如上图所示。句子级表示将整个句子编码为一个特征。如果短语基础数据中的某些句子具有多个短语，则会提取这些短语并丢弃其他单词。这样就消除了单词之间的影响，同时丢失了句子中的细粒度信息。词级表示能够在一次 forward 中编码多个类别名称，但会在类别之间引入不必要的依赖性，特别是当输入文本是多个类别名称以任意顺序串联时。如上图（b）所示，一些不相关的词在注意力过程中产生相互作用。为了避免不必要的单词交互，我们引入了注意力掩码来阻止不相关的类别名称之间的注意力，称为“子句子”级别表示。它消除了不同类别名称之间的影响，同时保留每个单词的特征以进行细粒度的理解。

### Loss

继之前类似 DETR 的工作之后，我们使用 L1 损失和 GIOU 损失进行边界框回归。我们遵循 GLIP 并使用预测对象和语言标记之间的对比损失进行分类。具体来说，我们将每个查询与文本特征进行点积，以预测每个文本标记的 logits，然后计算每个 logits 的 Focal Loss。框回归和分类结果首先用于预测和真实框之间的二分匹配。然后，我们使用相同的 loss 组件计算真实框和对应匹配的预测框的最终的 losses。遵循类似 DETR 的模型，我们在每个解码器层和编码器输出之后添加辅助损失。

## 消融实验

![image-20240227161138076](2303.05499%20Grounding%20DINO.assets/Ablations.png)

我们在本节中进行消融研究。我们提出了一种用于开放集对象检测的紧密融合基础模型和子句级文本提示。为了验证模型设计的有效性，我们删除了不同变体的一些融合块。结果如上表所示。所有模型均在具有 Swin-T 主干的 O365 上进行预训练。结果表明，每次融合都有助于最终的性能。编码器融合是最重要的设计。单词级文本提示的影响最小，但也很有帮助。语言引导的查询选择和文本交叉注意力分别对 LVIS 和 COCO 产生较大影响。

