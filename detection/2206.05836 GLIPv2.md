# 主要内容

我们提出了 GLIPv2，一种视觉语言理解的模型，它既服务于图像任务（例如：对象检测、实例分割），又服务于视觉语言（VL）理解任务（例如：VQA、图像字幕）。GLIPv2 优雅地将定位预训练和视觉语言预训练 (VLP) 与三个预训练任务统一起来：phrase grounding 作为检测任务的视觉语言重构，区域-单词对比学习作为新颖的区域-单词级别对比学习任务，以及掩码语言建模。这种统一不仅简化了之前的多阶段 VLP 程序，而且实现了定位和理解任务之间的互惠互利。实验结果表明，单个 GLIPv2 模型（所有模型权重共享）在各种定位和理解任务上实现了接近 SoTA 的性能。该模型还显示出（1）在开放词汇目标检测任务上强大的零样本和少样本适应性能，以及（2）在 VL 理解任务上的卓越基础能力。

![image-20240302093051679](2206.05836%20GLIPv2.assets/GLIPv2 architecture.png)

## Localization + VL understanding = grounded VL understanding

定位任务涉及定位和语义分类，其中分类可以使用 classification-to-matching 技巧转化为视觉语言理解问题。因此，我们将本地化任务重新表述为视觉语言基础任务 grounding，其中语言输入是作为类别名称拼接的合成句子。定位数据相应地转化为视觉语言标注数据。海量的视觉语言理解数据（图文对）可以通过自训练的方式轻松转化为视觉语言 grounding 数据。因此，GLIPv2有一个统一的预训练过程：所有任务数据都变成视觉语言 grounding 数据，并对GLIPv2进行预训练以进行基础语言理解理解。

GLIPv2 统一表述的核心是分类匹配技巧，它将任何特定于任务的固定词汇分类问题重新表述为与任务无关的开放词汇视觉语言匹配问题。最好的例子是将图像分类重新表述为 CLIP 中的图像文本匹配，这使得模型能够直接从原始图像文本数据中学习，并在开放词汇分类任务上实现强大的 zero-shot 能力。在 GLIPv2 中，我们用视觉语言匹配点积层替换传统单模态视觉模型中的每个语义分类线性层。

### 视觉语言理解任务。

Arch Π 是视觉语言理解任务中最流行的模型架构。给定跨模态融合表示 O 和 P，直接为各种视觉语言任务添加特定的轻量级任务头。例如，GLIPv2在文本特征P之上添加两层MLP作为掩码语言建模（MLM）头，以执行MLM预训练。

### （语言引导）物体检测和 phrase grounding。

继 GLIP 之后，GLIPv2 使用 classification-to-match 技巧来统一检测和 grounding。更具体地说，对于检测任务，我们只需简单地将分类 logits Scls = OW^T(其中 W 是框分类器的权重矩阵), 替换为一个与任务无关的 区域-词 的相似度 logits Sground = OP^T（其中文本特征 P 是来自与任务无关的语言编码器的类别嵌入）。如上图所示，目标检测和 phrase grounding 共享相同的输入/输出格式和模型架构。有关详细信息，请参阅 GLIP。它们唯一的区别是输入文本格式：（1）对于对象检测，文本输入是一串拼接的候选对象标签； (2)对于 phrase grounding，文本输入是自然语言句子。

### （语言引导）实例分割和语言引导的图像分割。

给定对象检测结果，通过添加实例分割头以将框中的每个像素分类为语义类。同样，GLIPv2 使用 classification-to-matching 技巧为标准实例分割任务和语言引导的图像分割任务生成统一的实例分割头，并利用这两种类型的数据进行预训练。这种 classification-to-matching 的技巧也可以应用于单模态 CV 模型（例如语义分割）中的许多其他语义分类头，从而将它们转移到语言引导的 CV 模型。

## 预训练**请看论文 3.2**

$$
L_{GLIPv2} = L_{ground} + L_{inter} + L_{mlm} \\

L_{ground} = L_{loc} + L_{intra}
$$



GLIPv2的预训练使用了3个loss：
1. phrase grounding 损失 $L_{ground}$ 来自对象检测任务的视觉语言重构
2. region-word 对比损失 $L_{inter}$ 来自新颖的 region-word 级对比学习任务。
3. 由 BERT 提粗的标准语言掩码损失 $L_{mlm}$​。

与检测任务中的损失类似，grounding 损失 $L_{ground}$ 有两部分：定位损失 $L_{loc}$ 通过边界框监督训练定位头，例如 RPN 损失、框回归损失/中心度损失； 图像内区域-词对齐损失 $L_{intra}$​，它本质上是每个区域的语义分类/检索损失。

![image-20240302112001988](2206.05836%20GLIPv2.assets/Loss.png)

GLIPv2 预训练损失：

图像内对齐损失 $L_{intra}$（右）在视觉理解融合后获取特征并计算每个图像-文本对的区域-单词对的损失；

图像间对比损失（左） $L_{inter}$ 在视觉理解融合之前获取特征，并计算一批图像-文本对中所有区域-单词对的损失。类别传播用于确定 $L_{inter}$ 目标矩阵的非对角区域的值。

### 图像内 region-word 对齐损失

给定一个图像-文本对（Img，Text），我们在跨模态融合 O 和 P 后获得图像和文本特征。图像内 region-word 对齐损失由以下计算
$$
L_{intra} = loss(OP^T, T)
$$
其中 $OP^T$ 是图像区域和单词标记之间的相似度得分，$T$ 是由真实标注确定的目标亲和度矩阵。损失函数损失通常是两阶段检测器的交叉熵损失和一阶段检测器的 focal loss。

### 图像间 region-word 对比损失(更强的视觉语言 grounding 任务)**创新**

在 GLIPv2 中，我们建议使用同一批次中其他图像文本对中的短语作为负例，这有效地将负例的数量增加到 1000 个数量级，而额外的计算成本几乎可以忽略不计。

图像间region-word 对比学习。GLIP 提出 phrase grounding 任务作为其预训练任务，我们认为这是一项简单的任务，并且没有充分利用数据信息。例如，在上图中的视觉语言 grounding 任务中， phrase grounding 任务仅要求模型将给定图像区域与文本输入中的三个短语之一进行匹配，即“绿色、粉色条纹或纯白色雨伞？”。这个三分之一的选择很简单，只需要理解颜色，但是在这个基础数据中丢失了很多信息：雨伞没有其他颜色，如黑色、黄色等； 这些区域中的物体是雨伞，但不是任何其他类别，如汽车、自行车等。从对比学习的角度来看，这个 phrase grounding 任务只有两个负样本。可以从此标注中创建更多负样本，从而实现更强的对比学习。在 GLIPv2 中，我们引入了新颖的  图像间region-word 区域词对比学习任务，该任务利用同一批次中其他句子中的短语作为潜在的负样本定，作为另一个更强的视觉语言 grounding 任务。这种新的区域词对比损失使 GLIPv2 能够学习更具辨别力的区域词特征，并展示了对所有下游任务的改进。

### 同时通过检测和图像文本做预训练

GLIPv2预训练数据采用图像-文本-目标三元组格式（Img，Text，T），其中目标亲和力矩阵 $T$ 包含 框-类别 定位标注。我们还使用大量图像文本对数据（Img、文本）来预训练 GLIPv2，通过使用 GLIP 预训练模型为文本中的短语生成 grounding boxes $\hat{T}$。人工标注的 OD/grounding 数据提供了高精度的定位监督，而海量的图文数据极大地提高了GLIPv2的概念多样性。

### 对分割头执行第二阶段的预训练

GLIPv2 通过使用实例分割和语言引导的图像分割数据集，对语言引导的分割头执行第二阶段的预训练，同时冻结模型的所有其他部分。

## 迁移学习到定位和视觉理解任务

### 相同的模型结构，为特殊任务训练指定的head(One model architecture for al)

通过使用（可选）特定于任务的头微调模型，可以将 GLIPv2 迁移到下游任务。1）对于检测和分割任务，不需要特定于任务的头，因为预训练架构本身就可以执行检测和分割。2）对于视觉理解任务：对于VQA，在序列开始标记的隐藏表示之上添加分类头； 对于标题生成，我们使用单向语言建模损失进行训练，这最大化了给定上下文的下一个单词的可能性。我们使用单向注意掩码并防止图像部分关注融合层中的文本。

### 提示微调(One set of weights for all)

通过调整提示词让模型应用于不同的任务

### Grounded 视觉理解

GLIPv2 还支持 grounding 视觉理解，我们在模型微调到下游视觉理解任务时保留 grounding 的能力。这增加了模型的可解释性。具体来说，我们首先使用预训练的 GLIP 模型将下游任务的视觉理解数据转换为 grounding 视觉理解数据。然后我们同时用下游任务头和 grounding 头训练模型。对于VQA任务，模型被训练来预测问题中的答案和标注明确/隐含的实体的位置； 对于字幕生成，模型经过训练以在给定上下文的情况下预测下一个单词，并对当前解码的单词进行定位。通过将定位任务调整为 grounded 视觉理解任务，并通过 grounding 能力增强视觉理解任务，我们有效地将每个任务转变为 grounding 的视觉理解任务。

## GLIPv2 vs GLIP

1) GLIP 表明 grounded 预训练可以改善定位。GLIPv2 进一步表明，grounded 的预训练可以提高视觉语言理解，从而形成用于定位和视觉语言理解的统一模型。
2) GLIPv2引入了 图像间region-word 对比损失，这是比GLIP中的预训练任务更强的 grounding 任务。所提出的损失可以被视为之前很流行的图像级对比学习的区域词级损失。
3) 在具有相同预训练数据的所有基准测试中，GLIPv2 的性能优于 GLIP。

