# 主要内容

## 主要结构

![image-20240301093648529](2112.03857%20GLIP.assets/GLIP architecture.png)

统一的检测和 phrase grounding 框架。与经典的物体检测模型不同，该模型为每个检测到的物体预测一个分类类别，我们将检测重新表述为 phrase grounding 任务，通过将每个区域/框与文本提示中的短语对齐。GLIP联合训练图像编码器和语言编码器，以预测区域和单词的正确配对。我们进一步添加了跨模态深度融合，以早期融合两种模态的信息，并学习语言感知的视觉表示。

通过将目标检测重新表述为 phrase grounding ，统一检测和 grounding 。这种重新表述改变了检测模型的输入：它不仅将图像作为输入，还将描述检测任务中所有候选类别的文本提示作为输入。例如，COCO目标检测的文本提示是一个由80个短语组成的文本字符串，即80个COCO目标类名称，由“.”连接，如上图（左）所示。任何目标检测模型都可以通过将框分类器中的目标分类分数转换为单词区域对齐分数来转换为 grounding 模型，即区域（或框）视觉特征和标记（或短语）语言特征的点积，如上图（右）所示。语言特征使用语言模型计算，这为新的检测（或 grounding ）模型提供了双编码结构。与只在最后一个点积层融合视觉和语言的CLIP不同，我们证明如上图（中）所示的GLIP应用的深度跨模态融合对于学习高质量的语言感知视觉表示和实现卓越的迁移学习性能至关重要。检测和 grounding 的统一也使我们能够使用这两种类型的数据进行预训练并使两个任务受益。在检测方面，由于 grounding 数据，视觉概念得到了显著丰富。在 grounding 方面，检测数据引入了更多的边界框注释并帮助训练一个新的 SoTA phrase grounding 模型。

### Object detection as phrase grounding.

我们没有将每个区域/框分类为 c 个类别，而是通过将每个区域与文本提示中的 c 个短语 grounding/对齐，将检测重新表述为 grounding 任务（见上图）。如何设计检测任务的文字提示？ 给定对象类 [person, bicycle, car, ..., toothbrush]，一种简单的方法是

`Prompt = “Detect: person, bicycle, car, ... , toothbrush”`

其中每个类名都是一个要标注的候选短语。通过提供这些类的更具表现力的描述 and/or 通过利用预先训练的语言模型的偏好，可以设计更好的提示。例如，当使用预训练的 BERT 模型来初始化我们的语言编码器 $Enc_L$​ 时，提示 “person. bicycle. car. ... . toothbrush” 比上面描述的更人性化的提示效果更好。

### Language-Aware Deep Fusion

在 CLIP 中，图像和文本由单独的编码器编码，仅在最后融合以计算对齐分数。我们将此类模型称为最后融合模型。在视觉语言模型中，视觉和语言特征的深度融合对于学习高效的 phrase grounding 模型是必要的。我们引入了图像和语言编码器之间的深度融合，它在最后几个编码层中融合了图像和文本信息，如上图（中）所示。

如下公式中（4）表示的是跨模态多头注意，（5）和（6）表示的是图像和文本的单模态计算

<img src="2112.03857%20GLIP.assets/Language-Aware Deep Fusion 1.png" alt="image-20240301115650485" style="zoom: 50%;" />

X-MHA 如下公式所示，注意只有 $O^{(q)}$、$P^{(q)}$、$O^{(v)}$、$P^{(v)}$，只有对应的 query 和 value，而 key 用另一个模态的 query 表示

Attn 只求一次即可，通过转置就可以转换为反向的注意力

<img src="2112.03857%20GLIP.assets/Language-Aware Deep Fusion 2.png" alt="image-20240301115702815" style="zoom:50%;" />

深度融合编码器（4）-（6）带来两个好处。1）提高了 phrase grounding 性能。2）它使学习到的视觉特征具有语言感知能力，因此模型的预测以文本提示为条件。这对于实现让一个模型服务于所有下游检测任务的目标至关重要。

## 利用海量图像文本数据扩展视觉概念

<img src="2112.03857%20GLIP.assets/Scaling up visual concepts with massive image-text data.png" alt="image-20240301094508646" style="zoom:50%;" />

利用海量图像文本数据扩展视觉概念。给定一个良好的基础模型（教师），我们可以通过自动生成海量图像文本配对数据的基础框来扩充 GLIP 预训练数据，其中名词短语由NLP解析器检测。因此，我们可以在2700万条基础数据上预训练我们的（学生）GLIP-Large 模型（GLIP-L），包括300万条人工标注的细粒度数据和2400万条网络爬取的图像文本对。对于2400万条图像文本对，有7810万条高置信度（> 0.5）短语框伪标注，其中5840万条是唯一的名词短语。我们在上图中展示了两个生成的框的真实例子。教师模型可以准确地定位一些可能难以理解的概念，如注射器、疫苗、美丽的加勒比海绿松石，甚至抽象词（视图）。在这种语义丰富数据上进行训练可以提供一个语义丰富的学生模型。相比之下，之前关于扩展检测数据的工作根本无法预测教师模型预定义词汇之外的概念。

首先，最好的 grounding 数据涵盖的视觉概念词汇量比现有检测数据要大得多。扩大检测词汇量的最大尝试仍然覆盖不超过 2,000 个类别。通过 grounding 数据，我们扩展了词汇量，几乎涵盖了 grounded 标注中出现的所有概念。

此外，我们没有扩大检测数据，而是展示了一种获得语义丰富数据的有前途的途径：扩大 grounding 数据集。我们使用受自我训练启发的简单方法。我们首先使用最好的（人工注释）检测数据集和 grounding 数据集对教师 GLIP 进行预训练。然后，我们使用这个教师模型来预测在网络上收集的图像文本数据的框，并由 NLP 解析器检测到名词短语。最后，使用最好的标注数据集和生成的伪标签数据集来训练学生模型。

为什么学生模型可能优于教师模型？ 虽然自我训练文献中的讨论仍然没有结论，但在视觉 grounding 的背景下，我们假设教师模型正在利用语言上下文和语言泛化能力来准确地理解它本身可能不知道的概念。例如，如果人工标注数据中没有出现某些概念，例如疫苗和绿松石，教师可能无法直接识别它们。然而，句法结构等丰富的语言上下文可以为教师模型执行“有根据的猜测”提供强有力的指导。如果模型能够定位一个小壁，则该模型可以定位疫苗； 如果它能找到加勒比海，它就能定位绿松石。当我们训练学生模型时，教师模型的“有根据的猜测”变成了“监督信号”，使学生模型能够学习疫苗和绿松石的概念。

## 迁移学习能力强大

使用 GLIP 进行迁移学习：一个模型适用全部任务。grounding 重构和语义丰富的预训练有利于领域转移。GLIP 可以转移到各种任务，只需很少甚至不需要额外的人工注释。当直接在 COCO 和 LVIS 数据集上评估 GLIP-L 模型时（在预训练期间没有看到 COCO 中的任何图像），它在 COCO val2017 和 LVIS val 上分别达到了 49.8 和 26.9 AP，超越了许多监督模型的基线。

使用图像文本对对于 COCO 数据集没有太大提升，但是对于 LVIS 数据集中的稀有类别的检测至关重要。并且在所有类别上都表现出强大的 zero-shot 性能，使用图像文本对训练的 GLIP 模型在 zero-shot 的评估中可以超过监督模型。

当对13个现有对象检测数据集进行评估时，涵盖细粒度物种检测、无人机视图检测和以自我为中心的检测等场景，我们将这种设置称为“野外对象检测”(ODinW)（第 5.1 节），GLIP 表现出出色的数据效率。

例如，zero-shot GLIP-L 优于在 Objects365 上预训练的 10 样本监督模型基线（Dynamic Head），而 1-shot GLIP-L 则可与完全监督 Dynamic Head 相媲美。此外，当特定于任务的标注可用时，无需调整整个模型，只需调整特定于任务的提示嵌入，同时保持模型参数不变。在这样的即时调优设置（第5.2节）下，一个 GLIP 模型可以同时在所有下游任务上表现良好，从而减少微调和部署成本。



## One Model for All Tasks

### 手动调整提示语句

![image-20240301144321642](2112.03857%20GLIP.assets/stingray.png)

由于 GLIP 执行语言感知定位，即 GLIP 的输出很大程度上取决于语言输入，因此我们提出了一种 GLIP 进行任务迁移的有效方法：对于任何新颖的类别，用户可以在文本提示中使用表达性描述， 添加属性或语言上下文，以注入领域知识并帮助 GLIP 迁移。例如，在上图的左侧，模型无法定位新实体“黄貂鱼”的所有出现位置。然而，通过将属性添加到提示中，即“扁平和圆形”，模型成功定位了所有出现的“黄貂鱼”。

### 提示微调

我们进一步考虑这样的设置：我们可以使用特定于任务的训练数据，但希望调整最少量的参数以便于部署。对于经典检测模型，Wang 报告了“线性微调”的有效性（即仅训练框回归和分类头）。GLIP 也可以是“线性微调”，我们只微调回归框以及区域和提示嵌入之间的投影层。由于语言感知深度融合，GLIP 支持更强大但仍然高效的传输策略：提示微调。对于 GLIP，由于每个检测任务只有一种语言提示（例如，对于所有图像，Pothole 的提示可能是“检测坑洞”），我们首先从语言主干得到提示嵌入 $P^0$，然后丢弃语言主干，仅将 $P^0$ 微调为特定于任务的输入。